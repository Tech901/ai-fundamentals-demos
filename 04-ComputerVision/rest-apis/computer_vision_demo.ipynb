{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Computer Vision API Demo\n",
    "\n",
    "This notebook demonstrates how to use Azure Computer Vision REST APIs for image analysis and vectorization.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Set the following environment variables:\n",
    "- `AZURE_CV_ENDPOINT` - Azure Computer Vision endpoint URL\n",
    "- `AZURE_CV_KEY1` - Azure Computer Vision API key\n",
    "- `AZURE_CV_REGION` - Azure region\n",
    "\n",
    "## A note on sample images\n",
    "\n",
    "The following resources are used in order to provide samples image inputs.\n",
    "\n",
    "1. [PlaceCats](https://placecats.com) \n",
    "2. Public Domain images from Unsplash: [see this collection](https://unsplash.com/collections/N2A19eP1Ers/cv-samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from rich import print_json\n",
    "\n",
    "# Load environment variables\n",
    "AZURE_CV_ENDPOINT = os.getenv(\"AZURE_CV_ENDPOINT\")\n",
    "AZURE_CV_KEY = os.getenv(\"AZURE_CV_KEY1\")\n",
    "AZURE_CV_REGION = os.getenv(\"AZURE_CV_REGION\")\n",
    "\n",
    "# Verify configuration\n",
    "if not all([AZURE_CV_ENDPOINT, AZURE_CV_KEY, AZURE_CV_REGION]):\n",
    "    print(\n",
    "        \"âš ï¸ Warning: Please set AZURE_CV_ENDPOINT, AZURE_CV_KEY1, and AZURE_CV_REGION environment variables\"\n",
    "    )\n",
    "else:\n",
    "    print(\"âœ… Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(\n",
    "    image_url: str, features: str, language: str = \"en\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze an image using Azure Computer Vision API.\n",
    "\n",
    "    Args:\n",
    "        image_url: URL of the image to analyze\n",
    "        features: Comma-separated list of visual features (tags, caption, denseCaptions, objects, read, people)\n",
    "        language: Language code (default: 'en')\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing the API response\n",
    "    \"\"\"\n",
    "    url = f\"{AZURE_CV_ENDPOINT}/computervision/imageanalysis:analyze\"\n",
    "\n",
    "    params = {\"features\": features, \"language\": language, \"api-version\": \"2024-02-01\"}\n",
    "\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": AZURE_CV_KEY,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    body = {\"url\": image_url}\n",
    "\n",
    "    response = requests.post(url, params=params, headers=headers, json=body)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def print_response(response: Dict[str, Any]):\n",
    "    \"\"\"Pretty print API response.\"\"\"\n",
    "    print_json(json.dumps(response, indent=2))\n",
    "\n",
    "\n",
    "def draw_bounding_boxes(\n",
    "    image_url: str,\n",
    "    people_result: Dict[str, Any],\n",
    "    confidence_threshold: float = 0.5,\n",
    "    box_color: str = \"red\",\n",
    "    box_width: int = 3,\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on an image for detected people.\n",
    "\n",
    "    Args:\n",
    "        image_url: URL of the image\n",
    "        people_result: The peopleResult data from the API response\n",
    "        confidence_threshold: Minimum confidence to draw a box (default: 0.5)\n",
    "        box_color: Color of the bounding box (default: 'red')\n",
    "        box_width: Width of the bounding box line (default: 3)\n",
    "\n",
    "    Returns:\n",
    "        PIL Image with bounding boxes drawn\n",
    "    \"\"\"\n",
    "    # Download the image\n",
    "    response = requests.get(image_url)\n",
    "    response.raise_for_status()\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Draw bounding boxes for each detected person\n",
    "    for person in people_result.get(\"values\", []):\n",
    "        confidence = person.get(\"confidence\", 0)\n",
    "\n",
    "        # Only draw boxes above confidence threshold\n",
    "        if confidence >= confidence_threshold:\n",
    "            bbox = person[\"boundingBox\"]\n",
    "            x, y, w, h = bbox[\"x\"], bbox[\"y\"], bbox[\"w\"], bbox[\"h\"]\n",
    "\n",
    "            # Draw rectangle\n",
    "            draw.rectangle([(x, y), (x + w, y + h)], outline=box_color, width=box_width)\n",
    "\n",
    "            # Add confidence label\n",
    "            label = f\"{confidence:.2%}\"\n",
    "            # Draw label background\n",
    "            text_bbox = draw.textbbox((x, y - 20), label)\n",
    "            draw.rectangle(text_bbox, fill=box_color)\n",
    "            draw.text((x, y - 20), label, fill=\"white\")\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_object_bounding_boxes(\n",
    "    image_url: str,\n",
    "    objects_result: Dict[str, Any],\n",
    "    confidence_threshold: float = 0.5,\n",
    "    box_color: str = \"blue\",\n",
    "    box_width: int = 3,\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on an image for detected objects.\n",
    "\n",
    "    Args:\n",
    "        image_url: URL of the image\n",
    "        objects_result: The objectsResult data from the API response\n",
    "        confidence_threshold: Minimum confidence to draw a box (default: 0.5)\n",
    "        box_color: Color of the bounding box (default: 'blue')\n",
    "        box_width: Width of the bounding box line (default: 3)\n",
    "\n",
    "    Returns:\n",
    "        PIL Image with bounding boxes drawn\n",
    "    \"\"\"\n",
    "    # Download the image\n",
    "    response = requests.get(image_url)\n",
    "    response.raise_for_status()\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Draw bounding boxes for each detected object\n",
    "    for obj in objects_result.get(\"values\", []):\n",
    "        # Get the highest confidence tag\n",
    "        tags = obj.get(\"tags\", [])\n",
    "        if not tags:\n",
    "            continue\n",
    "\n",
    "        # Sort by confidence and get the top tag\n",
    "        top_tag = max(tags, key=lambda t: t.get(\"confidence\", 0))\n",
    "        confidence = top_tag.get(\"confidence\", 0)\n",
    "        object_name = top_tag.get(\"name\", \"Unknown\")\n",
    "\n",
    "        # Only draw boxes above confidence threshold\n",
    "        if confidence >= confidence_threshold:\n",
    "            bbox = obj[\"boundingBox\"]\n",
    "            x, y, w, h = bbox[\"x\"], bbox[\"y\"], bbox[\"w\"], bbox[\"h\"]\n",
    "\n",
    "            # Draw rectangle\n",
    "            draw.rectangle([(x, y), (x + w, y + h)], outline=box_color, width=box_width)\n",
    "\n",
    "            # Add label with object name and confidence\n",
    "            label = f\"{object_name} ({confidence:.1%})\"\n",
    "            # Draw label background\n",
    "            text_bbox = draw.textbbox((x, y - 25), label)\n",
    "            draw.rectangle(text_bbox, fill=box_color)\n",
    "            draw.text((x, y - 25), label, fill=\"white\")\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_text_bounding_boxes(\n",
    "    image_url: str,\n",
    "    read_result: Dict[str, Any],\n",
    "    box_color: str = \"green\",\n",
    "    box_width: int = 2,\n",
    "    show_text: bool = True,\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on an image for detected text (OCR).\n",
    "\n",
    "    Args:\n",
    "        image_url: URL of the image\n",
    "        read_result: The readResult data from the API response\n",
    "        box_color: Color of the bounding box (default: 'green')\n",
    "        box_width: Width of the bounding box line (default: 2)\n",
    "        show_text: Whether to show text labels (default: True)\n",
    "\n",
    "    Returns:\n",
    "        PIL Image with bounding boxes drawn\n",
    "    \"\"\"\n",
    "    # Download the image\n",
    "    response = requests.get(image_url)\n",
    "    response.raise_for_status()\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Draw bounding boxes for each line of text\n",
    "    for block in read_result.get(\"blocks\", []):\n",
    "        for line in block.get(\"lines\", []):\n",
    "            text = line.get(\"text\", \"\")\n",
    "            bounding_polygon = line.get(\"boundingPolygon\", [])\n",
    "\n",
    "            if len(bounding_polygon) >= 4:\n",
    "                # Convert polygon points to tuple list\n",
    "                points = [(p[\"x\"], p[\"y\"]) for p in bounding_polygon]\n",
    "\n",
    "                # Draw polygon (handles rotated text better than rectangles)\n",
    "                draw.polygon(points, outline=box_color, width=box_width)\n",
    "\n",
    "                # Optionally add text label\n",
    "                if show_text and text:\n",
    "                    # Get the top-left corner for label placement\n",
    "                    x, y = points[0]\n",
    "\n",
    "                    # Draw text background\n",
    "                    text_bbox = draw.textbbox((x, y - 20), text)\n",
    "                    draw.rectangle(text_bbox, fill=box_color)\n",
    "                    draw.text((x, y - 20), text, fill=\"white\")\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def display_image(image_url: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Download and display an image from a URL.\n",
    "\n",
    "    Args:\n",
    "        image_url: URL of the image to display\n",
    "\n",
    "    Returns:\n",
    "        PIL Image object\n",
    "    \"\"\"\n",
    "    response = requests.get(image_url)\n",
    "    response.raise_for_status()\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_dense_captions(\n",
    "    image_url: str,\n",
    "    dense_captions_result: Dict[str, Any],\n",
    "    box_color: str = \"purple\",\n",
    "    box_width: int = 3,\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on an image for dense captions.\n",
    "\n",
    "    Args:\n",
    "        image_url: URL of the image\n",
    "        dense_captions_result: The denseCaptionsResult data from the API response\n",
    "        box_color: Color of the bounding box (default: 'purple')\n",
    "        box_width: Width of the bounding box line (default: 3)\n",
    "\n",
    "    Returns:\n",
    "        PIL Image with bounding boxes and captions drawn\n",
    "    \"\"\"\n",
    "    # Download the image\n",
    "    response = requests.get(image_url)\n",
    "    response.raise_for_status()\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Draw bounding boxes for each dense caption\n",
    "    for idx, caption_obj in enumerate(dense_captions_result.get(\"values\", [])):\n",
    "        text = caption_obj.get(\"text\", \"\")\n",
    "        confidence = caption_obj.get(\"confidence\", 0)\n",
    "        bbox = caption_obj.get(\"boundingBox\", {})\n",
    "\n",
    "        x, y, w, h = (\n",
    "            bbox.get(\"x\", 0),\n",
    "            bbox.get(\"y\", 0),\n",
    "            bbox.get(\"w\", 0),\n",
    "            bbox.get(\"h\", 0),\n",
    "        )\n",
    "\n",
    "        # Draw rectangle\n",
    "        draw.rectangle([(x, y), (x + w, y + h)], outline=box_color, width=box_width)\n",
    "\n",
    "        # Add caption text and confidence\n",
    "        label = f\"{text} ({confidence:.1%})\"\n",
    "\n",
    "        # Draw label background\n",
    "        text_bbox = draw.textbbox((x, y - 25), label)\n",
    "        draw.rectangle(text_bbox, fill=box_color)\n",
    "        draw.text((x, y - 25), label, fill=\"white\")\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyze Image: Tags\n",
    "\n",
    "Extract content tags from the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://placecats.com/300/200\"\n",
    "response = analyze_image(image_url, features=\"tags\", language=\"en\")\n",
    "print_response(response)\n",
    "display_image(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Image: Caption\n",
    "\n",
    "Generate a single caption describing the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://placecats.com/louie/300/200\"\n",
    "response = analyze_image(image_url, features=\"caption\", language=\"en\")\n",
    "print_response(response)\n",
    "display_image(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Image: Dense Captions\n",
    "\n",
    "Generate detailed captions for different regions of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://placecats.com/millie_neo/300/200\"\n",
    "response = analyze_image(image_url, features=\"denseCaptions\", language=\"en\")\n",
    "print_response(response)\n",
    "display_image(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Dense Captions with Bounding Boxes\n",
    "\n",
    "Dense captions provide detailed descriptions for different regions of the image. Let's visualize where each caption applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes with captions overlaid\n",
    "img_with_captions = draw_dense_captions(image_url, response[\"denseCaptionsResult\"])\n",
    "\n",
    "# Display the image\n",
    "img_with_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Image: Objects\n",
    "\n",
    "Detect and locate objects in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drink\n",
    "# image_url = \"https://images.unsplash.com/photo-1760662018512-e881f06d0fe0?q=80&w=1024\"\n",
    "\n",
    "# tools (ðŸ˜Ÿ)\n",
    "# image_url = \"https://images.unsplash.com/photo-1567361808960-dec9cb578182?q=80&w=1024\"\n",
    "\n",
    "# items in a purse\n",
    "# image_url = \"https://images.unsplash.com/photo-1602532360508-595f449c7c55?q=80&w=1024\"\n",
    "\n",
    "# desk\n",
    "image_url = \"https://images.unsplash.com/photo-1625461291092-13d0c45608b3?q=80&w=1024\"\n",
    "\n",
    "# produce\n",
    "# image_url = \"https://images.unsplash.com/photo-1635341083777-5f93a755e916?q=80&w=1024\"\n",
    "\n",
    "# Ramen\n",
    "# image_url = \"https://images.unsplash.com/photo-1504674900247-0877df9cc836?q=80&w=1024\"\n",
    "\n",
    "response = analyze_image(image_url, features=\"objects\", language=\"en\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Object Detection with Bounding Boxes\n",
    "\n",
    "Draw bounding boxes on the image to visualize detected objects. Each box is labeled with the object name and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes for objects detected with confidence >= 50%\n",
    "img_with_boxes = draw_object_bounding_boxes(\n",
    "    image_url, response[\"objectsResult\"], confidence_threshold=0.5\n",
    ")\n",
    "\n",
    "# Display the image\n",
    "img_with_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust parameters to customize the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all detections with custom styling\n",
    "img_all_objects = draw_object_bounding_boxes(\n",
    "    image_url,\n",
    "    response[\"objectsResult\"],\n",
    "    confidence_threshold=0.3,  # <--- Lower the threshold\n",
    "    box_color=\"yellow\",  # <--- Change the box color\n",
    "    box_width=8,  # <--- Change the box size\n",
    ")\n",
    "\n",
    "img_all_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Image: Read (OCR)\n",
    "\n",
    "Extract text from the image using Optical Character Recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text only\n",
    "image_url = \"https://images.unsplash.com/photo-1592496000931-e50d83df1286?q=80&w=1024\"\n",
    "\n",
    "# Placard\n",
    "# image_url = \"https://images.unsplash.com/photo-1495001258031-d1b407bc1776?q=80&w=1024\"\n",
    "response = analyze_image(image_url, features=\"read\", language=\"en\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize OCR Text Detection with Bounding Boxes\n",
    "\n",
    "Draw bounding boxes around detected text. The function uses polygon shapes to accurately represent rotated or skewed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes for detected text with labels\n",
    "img_with_text_boxes = draw_text_bounding_boxes(\n",
    "    image_url, response[\"readResult\"], show_text=True\n",
    ")\n",
    "\n",
    "# Display the image\n",
    "img_with_text_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also show just the bounding boxes without text labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes only (no text labels)\n",
    "img_boxes_only = draw_text_bounding_boxes(\n",
    "    image_url,\n",
    "    response[\"readResult\"],\n",
    "    show_text=False,\n",
    "    box_color=\"orange\",\n",
    "    box_width=3,\n",
    ")\n",
    "\n",
    "img_boxes_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Image: People\n",
    "\n",
    "Detect people in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# women with some drinks\n",
    "image_url = \"https://images.unsplash.com/photo-1532635241-17e820acc59f?q=80&w=1024\"\n",
    "\n",
    "# woman on roof\n",
    "# image_url = \"https://images.unsplash.com/photo-1589156288859-f0cb0d82b065?q=80&w=1024\"\n",
    "\n",
    "# couple  on a bench\n",
    "# image_url = \"https://images.unsplash.com/photo-1508963493744-76fce69379c0?q=80&w=1024\"\n",
    "\n",
    "# people in a crosswalk\n",
    "# image_url = \"https://images.unsplash.com/photo-1548783307-f63adc3f200b?q=80&w=1024\"\n",
    "\n",
    "response = analyze_image(image_url, features=\"people\", language=\"en\")\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize People Detection with Bounding Boxes\n",
    "\n",
    "Draw bounding boxes on the image to visualize detected people. By default, only shows detections with confidence >= 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes for people detected with confidence >= 50%\n",
    "img_with_boxes = draw_bounding_boxes(\n",
    "    image_url, response[\"peopleResult\"], confidence_threshold=0.5\n",
    ")\n",
    "\n",
    "# Display the image\n",
    "img_with_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust the confidence threshold to see more or fewer detections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all detections (including low confidence)\n",
    "img_all_detections = draw_bounding_boxes(\n",
    "    image_url,\n",
    "    response[\"peopleResult\"],\n",
    "    confidence_threshold=0.0,\n",
    "    box_color=\"green\",\n",
    "    box_width=2,\n",
    ")\n",
    "\n",
    "img_all_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vectorize Image\n",
    "\n",
    "Generate vector embeddings for the image, useful for image similarity search and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_image(\n",
    "    image_url: str, model_version: str = \"2023-04-15\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Vectorize an image using Azure Computer Vision API.\n",
    "\n",
    "    Args:\n",
    "        image_url: URL of the image to vectorize\n",
    "        model_version: Model version to use (default: '2023-04-15')\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing the API response with vector embeddings\n",
    "    \"\"\"\n",
    "    url = f\"{AZURE_CV_ENDPOINT}/computervision/retrieval:vectorizeImage\"\n",
    "\n",
    "    params = {\"model-version\": model_version, \"api-version\": \"2024-02-01\"}\n",
    "\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": AZURE_CV_KEY,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    body = {\"url\": image_url}\n",
    "\n",
    "    response = requests.post(url, params=params, headers=headers, json=body)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# Vectorize the image (using our produce image)\n",
    "image_url = \"https://images.unsplash.com/photo-1635341083777-5f93a755e916?q=80&w=1024\"\n",
    "response = vectorize_image(image_url, model_version=\"2023-04-15\")\n",
    "\n",
    "# Print response (showing vector dimensions)\n",
    "if \"vector\" in response:\n",
    "    vector = response[\"vector\"]\n",
    "    print(f\"Vector dimensions: {len(vector)}\")\n",
    "    print(f\"First 10 values: {vector[:10]}\")\n",
    "    print(f\"\\nFull response structure:\")\n",
    "    print(\n",
    "        json.dumps(\n",
    "            {\n",
    "                k: v if k != \"vector\" else f\"<{len(v)} dimensional vector>\"\n",
    "                for k, v in response.items()\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Combine Multiple Features\n",
    "\n",
    "You can analyze multiple features in a single API call by combining them with commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple features at once\n",
    "# woman in shopping cart.\n",
    "image_url = \"https://images.unsplash.com/photo-1643443578657-6170bfbb1920?q=80&w=1024\"\n",
    "response = analyze_image(image_url, features=\"tags,caption,objects\", language=\"en\")\n",
    "# print_response(response)\n",
    "\n",
    "caption_confidence = round(response[\"captionResult\"][\"confidence\"], 2)\n",
    "caption = response[\"captionResult\"][\"text\"]\n",
    "tags = [\n",
    "    f\"{d[\"name\"]} ({round(d[\"confidence\"], 2)})\"\n",
    "    for d in response[\"tagsResult\"][\"values\"]\n",
    "]\n",
    "\n",
    "print(f\"CAPTION: {caption_confidence}: {caption}\")\n",
    "print(f\"TAGS: {\"\\n\".join(tags)}\")\n",
    "\n",
    "img = draw_object_bounding_boxes(\n",
    "    image_url,\n",
    "    response[\"objectsResult\"],\n",
    "    confidence_threshold=0.5,\n",
    "    box_color=\"green\",\n",
    "    box_width=8,\n",
    ")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
